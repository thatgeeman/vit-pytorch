# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/00_patch.ipynb.

# %% auto 0
__all__ = ['PatchEmbedding']

# %% ../nbs/00_patch.ipynb 3
import torch
from torch import nn
import torch.functional as F
from torchvision import datasets
import numpy as np
import matplotlib.pyplot as plt

import yaml
from PIL import Image
from fastcore.basics import Path

# %% ../nbs/00_patch.ipynb 15
from fastcore.xtras import dict2obj
from fastcore.basics import filter_dict, filter_values, val2idx

# %% ../nbs/00_patch.ipynb 26
import torchvision.transforms as T

# %% ../nbs/00_patch.ipynb 47
from einops import repeat

# %% ../nbs/00_patch.ipynb 56
class PatchEmbedding(nn.Module):
    def __init__(self, config, channel_first=True) -> None:
        super().__init__()
        # patch related
        patch_size = config["patch"]["size"]
        in_ch = config["patch"]["in_ch"]
        embed_dim = config["patch"]["out_ch"] # out_ch
        seq_len = config["patch"]["n"] # n_patches
        # data related
        bs = config["data"]["bs"]
        
        self.conv2d = nn.Conv2d(
            in_channels=in_ch,
            out_channels=embed_dim,
            kernel_size=patch_size,
            stride=patch_size,
        )
        self.channel_first = channel_first
        self.flatten = nn.Flatten(start_dim=2, end_dim=-1)
        # class tokens are shared among batch items
        # 1, 1, embed_dim
        self.cls_token = nn.Parameter(torch.ones(1, 1, embed_dim), requires_grad=True)     
        # for all images in the batch what is the order of patches
        # 1, n_patches+1, embed_dim
        self.pos_embed = nn.Parameter(torch.ones(1, int(seq_len)+1, embed_dim), requires_grad=True)

    def forward(self, x):
        bs = x.shape[0]
        # repeat at each forward, not as learnable, need a class token with (1, 1, embed_dim)
        cls_tokens = repeat(self.cls_token, '1 1 d -> bs 1 d', bs = bs)
        if not self.channel_first:
            x = x.permute(0, 3, 1, 2)
        x = self.flatten(self.conv2d(x)).permute(0, 2, 1) # (bs, seq_len, embed_dim)
        x = torch.concat([cls_tokens, x], dim=1) + self.pos_embed
        return x

