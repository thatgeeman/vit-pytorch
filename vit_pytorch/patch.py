# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/00_patch.ipynb.

# %% auto 0
__all__ = ['CONFIG_PATH', 'DATA_PATH', 'PatchEmbedding']

# %% ../nbs/00_patch.ipynb 3
import torch
from torch import nn
import torch.functional as F
from torchvision import datasets
import numpy as np
import matplotlib.pyplot as plt

import yaml
from PIL import Image
from fastcore.basics import Path

# %% ../nbs/00_patch.ipynb 4
CONFIG_PATH = '../config.yml'
DATA_PATH = Path('../input') 

# %% ../nbs/00_patch.ipynb 15
from fastcore.xtras import dict2obj
from fastcore.basics import filter_dict, filter_values, val2idx

# %% ../nbs/00_patch.ipynb 26
import torchvision.transforms as T

# %% ../nbs/00_patch.ipynb 48
class PatchEmbedding(nn.Module):
    def __init__(self, channel_first=True) -> None:
        super().__init__()
        # patch related
        patch_size = config["patch"]["size"]
        in_ch = config["patch"]["in_ch"]
        out_ch = config["patch"]["out_ch"]
        n_patches = config["patch"]["n"]
        # data related
        bs = config["data"]["bs"]
        
        self.conv2d = nn.Conv2d(
            in_channels=in_ch,
            out_channels=out_ch,
            kernel_size=patch_size,
            stride=patch_size,
        )
        self.channel_first = channel_first
        self.flatten = nn.Flatten(start_dim=2, end_dim=-1)
        # for all patches in the image prepend the class token
        ones_token = torch.ones(bs, 1, out_ch) # bs, 1, embed_dim
        self.class_token = nn.Parameter(ones_token, requires_grad=True)
        # for all images in the batch what is the order of patches
        ones_embed = torch.ones(1, n_patches+1, out_ch) # 1, n_patches+1, embed_dim
        self.pos_embed = nn.Parameter(ones_embed, requires_grad=True)

    def forward(self, x):
        if not self.channel_first:
            x = x.permute(0, 3, 1, 2)
        # print(x.shape)
        x = self.conv2d(x)
        # print(x.shape)
        x = self.flatten(x) 
        x = x.permute(0, 2, 1) # (bs, patch_dim, embed_dim)
        x = torch.concat([self.class_token, x], dim=1) 
        x += self.pos_embed
        return x

