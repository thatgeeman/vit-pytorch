# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/01_encoder.ipynb.

# %% auto 0
__all__ = ['MultiheadSelfAttn', 'MLPBlock', 'TransformerEncoderLayer', 'TransformerEncoder']

# %% ../nbs/01_encoder.ipynb 3
import torch
from torch import nn
import torch.functional as F
from torchvision import datasets
import numpy as np

import yaml
from fastcore.basics import Path

# %% ../nbs/01_encoder.ipynb 19
import torchvision.transforms as T

# %% ../nbs/01_encoder.ipynb 23
from .patch import PatchEmbedding

# %% ../nbs/01_encoder.ipynb 33
class MultiheadSelfAttn(nn.Module):
    def __init__(self, config) -> None:
        super().__init__()
        self.seq_len = config['patch']['n']
        self.embed_dim = config['patch']['out_ch']
        self.num_heads = config['encoder']['msa_heads']
        self.ln = nn.LayerNorm(normalized_shape=embed_dim)
        self.msa = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=num_heads)
    
    def forward(self, x):
        x_ln = self.ln(x)
        x_msa, _ = self.msa(x_ln, x_ln, x_ln)
        return x_msa + x

# %% ../nbs/01_encoder.ipynb 36
class MLPBlock(nn.Module):
    def __init__(self, config) -> None:
        super().__init__()
        embed_dim = config["patch"]["out_ch"]
        mlp_size = config["encoder"]["mlp_size"]
        dropout = config["encoder"]["mlp_dropout"]
        self.ln = nn.LayerNorm(normalized_shape=embed_dim)
        self.linear = nn.Sequential(
            nn.Linear(in_features=embed_dim, out_features=mlp_size),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(in_features=mlp_size, out_features=embed_dim),
            nn.Dropout(dropout)
        )
    def forward(self, x):
        x_linear = self.linear(self.ln(x))
        return x_linear + x


# %% ../nbs/01_encoder.ipynb 39
class TransformerEncoderLayer(nn.Module):
    def __init__(self, config) -> None:
        super().__init__()
        self.msa_block = MLPBlock(config)
        self.mlp_block = MLPBlock(config)
    
    def forward(self, x):
        x = self.msa_block(x)
        return self.mlp_block(x)

# %% ../nbs/01_encoder.ipynb 42
class TransformerEncoder(nn.Module):
    def __init__(self, config) -> None:
        super().__init__()
        n_layers = config["encoder"]["layers"]
        self.transformers = nn.Sequential(
            *[TransformerEncoderLayer(config) for _ in range(n_layers)]
        )

    def forward(self, x):
        x = self.transformers(x)
        return x

