{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ViT model\n",
    "\n",
    "> Putting together patch embeddings and transformer encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.functional as F\n",
    "from torchvision import datasets\n",
    "import numpy as np\n",
    "\n",
    "import yaml\n",
    "from fastcore.basics import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_PATH = '../config.yml'\n",
    "DATA_PATH = Path('../input') "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load parameters from the config file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = yaml.safe_load(open(CONFIG_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = datasets.CIFAR10(DATA_PATH, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, targets = dset.data, dset.targets\n",
    "len(images), len(targets)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare a small batch of images to test the image processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample a bunch of points and select those as indices of the image for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_idx = np.random.randint(low=0, high=len(images), size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corresponding labels\n",
    "targets = [targets[t] for t in image_idx]\n",
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = config[\"model\"][\"n_classes\"]\n",
    "n_classes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting together PatchEmbedding and TransformerEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from vit_pytorch.patch import PatchEmbedding\n",
    "from vit_pytorch.encoder import TransformerEncoder\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = torch.Tensor(images[image_idx])\n",
    "images = images/255.\n",
    "hw = config['data']['hw']\n",
    "augs = T.Resize(hw)\n",
    "\n",
    "images = augs(images.permute(0, 3, 1, 2))\n",
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, config) -> None:\n",
    "        super().__init__()\n",
    "        n_classes = config[\"model\"][\"n_classes\"]\n",
    "        training = config[\"model\"][\"training\"]\n",
    "        emb_dim = config[\"patch\"][\"out_ch\"]\n",
    "        dropout = config[\"model\"][\"clf_dropout\"]\n",
    "        hidden_units = config[\"model\"][\"clf_hidden_units\"]\n",
    "        self.patch_embedding = PatchEmbedding(config)\n",
    "        self.transformer_encoder = TransformerEncoder(config)\n",
    "        # classification head\n",
    "        self.ln = nn.LayerNorm(emb_dim)\n",
    "        mlp_layers = (\n",
    "            [\n",
    "                nn.Linear(emb_dim, hidden_units),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(hidden_units, n_classes),\n",
    "            ]\n",
    "            if training\n",
    "            else [nn.Linear(emb_dim, n_classes)]\n",
    "        )\n",
    "        self.mlp = nn.Sequential(self.ln, *mlp_layers)\n",
    "        self.representation_ = None\n",
    "        self.class_token_ = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embedding(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        self.representation_ = x[:, 1:, :] # learned representation\n",
    "        # ? In lucidrains implementation, why is class token same in vision transformer and repeated in bs\n",
    "        # ? In my implementation, I initialized classtoken for each image, pass only the class token through the mlp head (bs, 1, 768)\n",
    "        # this is the first item that was concatenated in the patch embedding\n",
    "        self.class_token_ = self.patch_embedding.class_token\n",
    "        print(self.class_token_.shape)\n",
    "        x = self.mlp(self.class_token_)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit = VisionTransformer(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit(images) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit.representation_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit.class_token_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vit",
   "language": "python",
   "name": "vit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
