[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "vit-pytorch",
    "section": "",
    "text": "Add patch embeddings\nAdd transformer encoder layer\nAdd transformer encoder (multiple layers)\nUnderstand why repeating class token instead of setting the parameter with shape (bs, 1, embed_dim)\nAttention dropout\nEmbedding dropout\nMLP dropout (in encoder)\nAdd classification head\nComplete ViT-Base\nMake named layers to make torchvision compatible\nAdd training scripts"
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "vit-pytorch",
    "section": "Install",
    "text": "Install\npip install vit_pytorch"
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "vit-pytorch",
    "section": "How to use",
    "text": "How to use\nLoad a config.yml file and pass to ViT module to modify architecture parameters."
  },
  {
    "objectID": "patch.html",
    "href": "patch.html",
    "title": "Prepare patches",
    "section": "",
    "text": "CONFIG_PATH = '../config.yml'\nDATA_PATH = Path('../input')\n\nLoad parameters from the config file.\n\nconfig = yaml.safe_load(open(CONFIG_PATH))\n\n\ndset = datasets.CIFAR10(DATA_PATH, download=True, train=True)\n\nFiles already downloaded and verified\n\n\n\nimages, targets = dset.data, dset.targets\nlen(images), len(targets)\n\n(50000, 50000)\n\n\nPrepare a small batch of images to test the image processing.\n\nimages.shape\n\n(50000, 32, 32, 3)\n\n\nSample a bunch of points and select those as indices of the image for training.\n\nimage_idx = np.random.randint(low=0, high=len(images), size=3)\n\n\n# corresponding labels\ntargets = [targets[t] for t in image_idx]\ntargets\n\n[3, 2, 4]\n\n\nWhat are the classes we are dealing with?\n\n# dict2obj lets you dotify dicts or nested dicts\nclsidx = dict2obj(dset.class_to_idx)\nclsidx.bird\n\n2\n\n\n\n# filter the dict based on a function that checks k and v\nfilter_dict(clsidx, lambda k,v: v in targets)\n\n{'bird': 2, 'cat': 3, 'deer': 4}\n\n\n\n# filters based on just the values\nfilter_values(clsidx, lambda v: v in targets)\n\n{'bird': 2, 'cat': 3, 'deer': 4}\n\n\n\ntargets[0]\n\n3\n\n\n\nclsidx\n\n{ 'airplane': 0,\n  'automobile': 1,\n  'bird': 2,\n  'cat': 3,\n  'deer': 4,\n  'dog': 5,\n  'frog': 6,\n  'horse': 7,\n  'ship': 8,\n  'truck': 9}\n\n\n\nin_ch = config[\"patch\"][\"in_ch\"]\nout_ch = config[\"patch\"][\"out_ch\"]\n\n\n# size of each small patch\npatch_size = config['patch']['size']\npatch_size\n\n16\n\n\n\nimages.shape[1:]\n\n(32, 32, 3)\n\n\n\nimages = torch.Tensor(images[image_idx])\nimages = images/255.\nimages.shape\n\ntorch.Size([3, 32, 32, 3])\n\n\nIncrease image size to match with ViT paper \\(224\\times 224\\)\n\nhw = config['data']['hw']\naugs = T.Resize(hw)\naugs\n\nResize(size=[224, 224], interpolation=bilinear, max_size=None, antialias=None)\n\n\n\nimages = augs(images.permute(0, 3, 1, 2))\nimages.shape\n\ntorch.Size([3, 3, 224, 224])\n\n\n\nn_channels, height, width = images.shape[1:]\nprint(f\"image height: {height}, width: {width}, channels: {n_channels}\")\nassert height==width\n\nimage height: 224, width: 224, channels: 3\n\n\nNumber of patches is also called the sequence length in the original Transformers paper.\n\nn_patches = (height*width)/(patch_size**2)\nprint(f\"number of {patch_size}x{patch_size} patches in an image of shape {images.shape[1:]}: {n_patches}\")\n\nnumber of 16x16 patches in an image of shape torch.Size([3, 224, 224]): 196.0\n\n\n\nshape_sequence = (n_patches, (patch_size**2)*in_ch)\nprint(f\"shape of flattened 2D sequence: {shape_sequence}\")\n\nshape of flattened 2D sequence: (196.0, 768)\n\n\n\nPrepare flattened 2D sequence\nDisplay a sample image with title.\n\nidx=2\n\n\nplt.figure(figsize=(2, 2))\nplt.imshow(images[idx].permute(1, 2, 0))\nplt.axis('off')\nlabel = filter_values(clsidx, lambda v: v is targets[idx])\nplt.title(label=label)\n\nText(0.5, 1.0, \"{'deer': 4}\")\n\n\n\n\n\nUse a convolutional layer to prepare a patched image.\n\nconv2d = nn.Conv2d(in_channels=in_ch, out_channels=out_ch, kernel_size=patch_size, stride=patch_size)\n\nPyTorch requires images in BCHW format.\n\nimages.shape\n\ntorch.Size([3, 3, 224, 224])\n\n\n\npatched_image = conv2d(images)\npatched_image.shape\n\ntorch.Size([3, 768, 14, 14])\n\n\n\npatched_image = patched_image.flatten(start_dim=2, end_dim=-1)\npatched_image.shape # flatten to 2D\n# permute to match the expected shape\npatched_image = patched_image.permute(0, 2, 1)\npatched_image.shape\n\ntorch.Size([3, 196, 768])\n\n\n\n\nPrepare class embedding\nThe goal is to prepend to the flattened sequence a new item that encodes the class. Inorder to create class embedding, we need to know the embedding dimension as well since each of the embeddings is associated to the class as well.\n\nbs, seq_len, embed_dim = patched_image.shape\nprint(f\"shape of embed dim (D): {embed_dim}\")\nprint(f\"shape of sequence length (N): {seq_len}\")\n\nshape of embed dim (D): 768\nshape of sequence length (N): 196\n\n\n\n# for all patches in the image prepend a learnable class token \n# this is a common token, not separated by items in batch\nclass_token = nn.Parameter(torch.ones(1, 1, embed_dim), requires_grad=True)\nclass_token.shape\n\ntorch.Size([1, 1, 768])\n\n\n\nclass_token.shape, patched_image.shape\n\n(torch.Size([1, 1, 768]), torch.Size([3, 196, 768]))\n\n\n\n# class tokens are shared among batch items\nclass_tokens = repeat(class_token, '1 1 d -> bs 1 d', bs = bs)\nclass_tokens.shape\n\ntorch.Size([3, 1, 768])\n\n\n\ntorch.concat([class_tokens, patched_image], dim=1)\n\ntensor([[[ 1.0000e+00,  1.0000e+00,  1.0000e+00,  ...,  1.0000e+00,\n           1.0000e+00,  1.0000e+00],\n         [-1.9330e-01,  5.3230e-01,  4.1072e-01,  ..., -7.1345e-02,\n          -3.6855e-02, -5.7116e-03],\n         [-2.3731e-01,  6.1622e-01,  4.8126e-01,  ..., -3.7821e-02,\n          -2.9133e-02, -4.7400e-02],\n         ...,\n         [-8.0137e-02,  1.7678e-01,  1.2428e-01,  ..., -3.5896e-02,\n          -3.9498e-02,  9.8821e-03],\n         [-5.6294e-02,  1.7154e-01,  9.5655e-02,  ..., -4.0395e-02,\n          -2.8825e-02, -8.3997e-04],\n         [-5.4865e-02,  1.5447e-01,  8.8190e-02,  ..., -2.9878e-02,\n          -3.1163e-02, -7.8464e-04]],\n\n        [[ 1.0000e+00,  1.0000e+00,  1.0000e+00,  ...,  1.0000e+00,\n           1.0000e+00,  1.0000e+00],\n         [-2.1967e-02,  1.6781e-01,  1.1162e-01,  ..., -1.2469e-01,\n          -8.1662e-02,  1.2122e-01],\n         [-1.9772e-02,  1.7202e-01,  1.1616e-01,  ..., -1.3429e-01,\n          -8.4445e-02,  1.2937e-01],\n         ...,\n         [-2.6714e-01,  4.9820e-01,  4.5177e-01,  ...,  8.2063e-02,\n           5.1341e-02, -2.3969e-01],\n         [-9.8749e-02,  1.2397e-01,  9.3057e-02,  ...,  3.5830e-02,\n          -2.1807e-03, -5.0337e-02],\n         [-9.7703e-02,  2.9660e-01,  1.7762e-01,  ...,  4.5148e-02,\n          -2.6980e-03, -9.4421e-02]],\n\n        [[ 1.0000e+00,  1.0000e+00,  1.0000e+00,  ...,  1.0000e+00,\n           1.0000e+00,  1.0000e+00],\n         [-1.3643e-02,  7.4828e-02,  1.8845e-02,  ...,  3.0374e-03,\n          -2.4055e-02, -1.7881e-02],\n         [-6.4193e-02,  1.5682e-01,  1.2541e-01,  ..., -4.4255e-03,\n          -1.8938e-02, -4.9337e-02],\n         ...,\n         [-4.5829e-02,  1.6316e-01,  1.0169e-01,  ..., -7.5816e-03,\n          -1.7019e-02, -3.2718e-02],\n         [-9.3686e-02,  2.6215e-01,  1.8407e-01,  ...,  1.8344e-02,\n          -8.4813e-03, -5.2504e-02],\n         [-9.8182e-02,  2.9226e-01,  1.7385e-01,  ...,  2.4724e-02,\n           1.0079e-02, -8.5595e-02]]], grad_fn=<CatBackward0>)\n\n\n\npatched_image = torch.concat([class_tokens, patched_image], dim=1)\npatched_image.shape\n\ntorch.Size([3, 197, 768])\n\n\n\n\nPrepare positional embedding\nThis should now be same for all images in the batch. Add positional embedding for each patch.\n\n# +1 for the extra class token added above\npos_token = nn.Parameter(torch.ones(1, int(seq_len)+1, embed_dim), requires_grad=True)\npos_token.shape\n\ntorch.Size([1, 197, 768])\n\n\nAdd the positional embedding to create the input \\(z_l^0\\)\n\npatched_image += pos_token\n\n\n\nPatchEmbedding Module\nPyTorch module that does all of the above.\n\nsource\n\nPatchEmbedding\n\n PatchEmbedding (config, channel_first=True)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nPatchEmbedding(config, channel_first=True)(images).shape\n\ntorch.Size([3, 197, 768])"
  },
  {
    "objectID": "model.html",
    "href": "model.html",
    "title": "ViT model",
    "section": "",
    "text": "CONFIG_PATH = '../config.yml'\nDATA_PATH = Path('../input')\n\nLoad parameters from the config file.\n\nconfig = yaml.safe_load(open(CONFIG_PATH))\n\n\ndset = datasets.CIFAR10(DATA_PATH, download=True)\n\nFiles already downloaded and verified\n\n\n\nimages, targets = dset.data, dset.targets\nlen(images), len(targets)\n\n(50000, 50000)\n\n\nPrepare a small batch of images to test the image processing.\n\nimages.shape\n\n(50000, 32, 32, 3)\n\n\nSample a bunch of points and select those as indices of the image for training.\n\nimage_idx = np.random.randint(low=0, high=len(images), size=3)\n\n\n# corresponding labels\ntargets = [targets[t] for t in image_idx]\ntargets\n\n[3, 6, 2]\n\n\n\nn_classes = config[\"model\"][\"n_classes\"]\nn_classes\n\n10\n\n\n\nPutting together PatchEmbedding and TransformerEncoder\n\nimages = torch.Tensor(images[image_idx])\nimages = images/255.\nhw = config['data']['hw']\naugs = T.Resize(hw)\n\nimages = augs(images.permute(0, 3, 1, 2))\nimages.shape\n\n/Users/gg/.local/share/virtualenvs/vit-pytorch-u3xJdwPd/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n  warnings.warn(\n\n\ntorch.Size([3, 3, 224, 224])\n\n\n\nsource\n\nVisionTransformer\n\n VisionTransformer (config)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nvit = VisionTransformer(config)\n\n\nouts = vit(images)\nouts.shape\n\ntorch.Size([3, 10])\n\n\n\nvit.embeddings_.shape\n\ntorch.Size([3, 196, 768])\n\n\n\nvit.cls_tokens_.shape\n\ntorch.Size([3, 768])"
  },
  {
    "objectID": "encoder.html",
    "href": "encoder.html",
    "title": "Transformer Encoder",
    "section": "",
    "text": "CONFIG_PATH = '../config.yml'\nDATA_PATH = Path('../input')\nLoad parameters from the config file.\nPrepare a small batch of images to test the image processing.\nSample a bunch of points and select those as indices of the image for training.\nIncrease image size to match with ViT paper \\(224\\times 224\\)"
  },
  {
    "objectID": "encoder.html#prepare-msa-block",
    "href": "encoder.html#prepare-msa-block",
    "title": "Transformer Encoder",
    "section": "Prepare MSA block",
    "text": "Prepare MSA block\n\nsource\n\nMultiheadSelfAttn\n\n MultiheadSelfAttn (config)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nx = MultiheadSelfAttn(config)(patch_embed)\nx.shape\n\ntorch.Size([3, 197, 768])"
  },
  {
    "objectID": "encoder.html#prepare-mlp-block",
    "href": "encoder.html#prepare-mlp-block",
    "title": "Transformer Encoder",
    "section": "Prepare MLP block",
    "text": "Prepare MLP block\n\nsource\n\nMLPBlock\n\n MLPBlock (config)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nx = MLPBlock(config)(x)\nx.shape\n\ntorch.Size([3, 197, 768])"
  },
  {
    "objectID": "encoder.html#transformer-encoder",
    "href": "encoder.html#transformer-encoder",
    "title": "Transformer Encoder",
    "section": "Transformer Encoder",
    "text": "Transformer Encoder\n\nsource\n\nTransformerEncoderLayer\n\n TransformerEncoderLayer (config)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nout = TransformerEncoderLayer(config)(patch_embed)\nout.shape\n\ntorch.Size([3, 197, 768])"
  },
  {
    "objectID": "encoder.html#multilayered-transformer-encoder",
    "href": "encoder.html#multilayered-transformer-encoder",
    "title": "Transformer Encoder",
    "section": "Multilayered Transformer Encoder",
    "text": "Multilayered Transformer Encoder\n\nsource\n\nTransformerEncoder\n\n TransformerEncoder (config)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nTransformerEncoder(config)(patch_embed).shape\n\ntorch.Size([3, 197, 768])"
  }
]